{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2TOZ37aQnHH",
        "outputId": "712382fe-0d94-47d7-a193-9df742b0ec36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "is_0wotMV1yD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import skimage.io as io\n",
        "from skimage.filters import threshold_otsu\n",
        "from skimage.morphology import erosion, dilation\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "%matplotlib inline\n",
        "\n",
        "# Set the path to the folder containing the images\n",
        "\n",
        "folder_path='drive/MyDrive/samples'\n",
        "\n",
        "# Set the window size for splitting the images\n",
        "\n",
        "window_width = 20\n",
        "window_height = 100\n",
        "\n",
        "# Function to preprocess an image using erosion and dilation\n",
        "\n",
        "def preprocess_image(image):\n",
        "    # gray image\n",
        "    img = io.imread(image, as_gray=True)\n",
        "    #plt.imshow(img, 'gray', aspect='auto')\n",
        "    # binary image\n",
        "    threshold = threshold_otsu(img)\n",
        "    img_binary = img > threshold\n",
        "    kernel = np.ones((2, 2), np.uint8)\n",
        "    # dilated 1\n",
        "    img_dilated = dilation(img_binary, kernel)\n",
        "    # erosion 1\n",
        "    img_eroded = erosion(img_dilated)\n",
        "\n",
        "    kernel = np.ones((3, 1), np.uint8)\n",
        "    # dilated 2\n",
        "    img_dilated = dilation(img_binary, kernel)\n",
        "    # erosion 2\n",
        "    img_eroded = erosion(img_dilated)\n",
        "\n",
        "    #dilation 3 \n",
        "    kernel = np.ones((4,1), np.uint8)\n",
        "    img_dilated_2 = dilation(img_eroded, kernel)\n",
        "    return img_dilated_2\n",
        "\n",
        "\n",
        "# Load and preprocess the images\n",
        "\n",
        "preprocessed_images = []\n",
        "preprocessed_character_images = []\n",
        "\n",
        "# For storing the characters on each character image \n",
        "labels=[]\n",
        "\n",
        "for filename in os.listdir(folder_path):\n",
        "    image_path = os.path.join(folder_path, filename)\n",
        "    image = preprocess_image(image_path)\n",
        "    preprocessed_images.append(image)\n",
        "\n",
        "    # Split each image into five character images\n",
        "    height, width = image.shape[:2]\n",
        "    for i in range(5):\n",
        "        start_x = i * window_width\n",
        "        end_x = (i + 1) * window_width\n",
        "        character_image = image[:, start_x:end_x]\n",
        "        preprocessed_character_images.append(character_image)\n",
        "        labels.append(filename[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Xf0lS_mmHCyF"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Flatten the preprocessed images into 1D arrays\n",
        "feature_vectors = [image.flatten() for image in preprocessed_character_images]\n",
        "feature_vectors = np.array(feature_vectors)\n",
        "\n",
        "# Convert labels to numpy array\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Split the data into train and validation sets\n",
        "train_x,val_x,train_y,val_y = train_test_split(\n",
        "    feature_vectors,labels, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "etxbSiwwHGuA",
        "outputId": "16248c50-c2f4-40ee-b0c0-0614b7f9f02d"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Train Logistic Regression classifier\n",
        "lr_classifier = LogisticRegression()\n",
        "lr_classifier.fit(train_x, train_y)\n",
        "\n",
        "# Train Support Vector Machine classifier\n",
        "svm_classifier = SVC()\n",
        "svm_classifier.fit(train_x, train_y)\n",
        "\n",
        "# Train Random Forest classifier\n",
        "rf_classifier = RandomForestClassifier()\n",
        "rf_classifier.fit(train_x, train_y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Dekwj6SHJWV",
        "outputId": "06cde9e3-ab77-4ebd-afd9-0b4c235875c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression Performance:\n",
            "Accuracy: 0.6232876712328768\n",
            "Precision: 0.6343438869245378\n",
            "Recall: 0.6232876712328768\n",
            "F1-Score: 0.6209229817640419\n",
            "Support Vector Machine Performance:\n",
            "Accuracy: 0.7157534246575342\n",
            "Precision: 0.7596878377890673\n",
            "Recall: 0.7157534246575342\n",
            "F1-Score: 0.7188536308417801\n",
            "Random Forest Performance:\n",
            "Accuracy: 0.7414383561643836\n",
            "Precision: 0.7734579273651232\n",
            "Recall: 0.7414383561643836\n",
            "F1-Score: 0.7447581534061665\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Evaluate on validation data\n",
        "lr_predictions = lr_classifier.predict(val_x)\n",
        "svm_predictions = svm_classifier.predict(val_x)\n",
        "rf_predictions = rf_classifier.predict(val_x)\n",
        "\n",
        "# Calculate performance metrics\n",
        "lr_accuracy = accuracy_score(val_y, lr_predictions)\n",
        "lr_precision = precision_score(val_y, lr_predictions, average='weighted')\n",
        "lr_recall = recall_score(val_y, lr_predictions, average='weighted')\n",
        "lr_f1 = f1_score(val_y, lr_predictions, average='weighted')\n",
        "\n",
        "svm_accuracy = accuracy_score(val_y, svm_predictions)\n",
        "svm_precision = precision_score(val_y, svm_predictions, average='weighted')\n",
        "svm_recall = recall_score(val_y, svm_predictions, average='weighted')\n",
        "svm_f1 = f1_score(val_y, svm_predictions, average='weighted')\n",
        "\n",
        "rf_accuracy = accuracy_score(val_y, rf_predictions)\n",
        "rf_precision = precision_score(val_y, rf_predictions, average='weighted')\n",
        "rf_recall = recall_score(val_y, rf_predictions, average='weighted')\n",
        "rf_f1 = f1_score(val_y, rf_predictions, average='weighted')\n",
        "\n",
        "# Print the performance metrics\n",
        "print(\"Logistic Regression Performance:\")\n",
        "print(\"Accuracy:\", lr_accuracy)\n",
        "print(\"Precision:\", lr_precision)\n",
        "print(\"Recall:\", lr_recall)\n",
        "print(\"F1-Score:\", lr_f1)\n",
        "\n",
        "print(\"Support Vector Machine Performance:\")\n",
        "print(\"Accuracy:\", svm_accuracy)\n",
        "print(\"Precision:\", svm_precision)\n",
        "print(\"Recall:\", svm_recall)\n",
        "print(\"F1-Score:\", svm_f1)\n",
        "\n",
        "\n",
        "print(\"Random Forest Performance:\")\n",
        "print(\"Accuracy:\", rf_accuracy)\n",
        "print(\"Precision:\", rf_precision)\n",
        "print(\"Recall:\", rf_recall)\n",
        "print(\"F1-Score:\", rf_f1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "mr6iKvKWNalb"
      },
      "outputs": [],
      "source": [
        "# ANSWER-1\n",
        "# Here, random forest yields the best accuracy on the validation set.\n",
        "# Accuracy is a commonly used metric to evaluate the performance of classification\n",
        "# algorithms, especially when the classes are balanced. However, there are cases where\n",
        "# accuracy alone, though, may not always be sufficient to assess a model's performance.\n",
        "# In those cases, we would prefer using “F1 score” to evaluate the model’s performance.\n",
        "# The F1 score is the harmonic mean of precision and recall. It provides a balanced\n",
        "# evaluation of an model’s performance, especially in scenarios where both precision and\n",
        "# recall are important.\n",
        "# Precision measures the proportion of correctly predicted positive instances out of all\n",
        "# instances predicted as positive, while recall measures the proportion of correctly\n",
        "# predicted positive instances out of all actual positive instances. These metrics are\n",
        "# particularly useful when dealing with imbalanced datasets, where the number of\n",
        "# instances in one class is significantly higher than the other.\n",
        "\n",
        "# ANSWER-3 \n",
        "# Precision, recall and F1-score are shown above for all the three models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LD1IsC-VHZG-",
        "outputId": "5cd71cc3-a93b-43e7-fbfb-4d8822fe1054"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7636986301369864\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Define the parameter grid for grid search\n",
        "param_grid = {\n",
        "    'n_estimators': [300],  # Number of trees\n",
        "    'max_depth': [15],  # Maximum depth of trees\n",
        "    'max_features': ['sqrt', 'log2']  # Maximum number of features to consider\n",
        "}\n",
        "\n",
        "# Create a Random Forest classifier\n",
        "rf_classifier = RandomForestClassifier()\n",
        "\n",
        "# Perform grid search\n",
        "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5)\n",
        "grid_search.fit(train_x, train_y)\n",
        "\n",
        "# Get the best hyperparameters found by grid search\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Train the model with the best hyperparameters\n",
        "best_rf_classifier = RandomForestClassifier(**best_params)\n",
        "best_rf_classifier.fit(train_x, train_y)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = best_rf_classifier.predict(val_x)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(val_y, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "36Pz5Z8FOtYJ"
      },
      "outputs": [],
      "source": [
        "# ANSWER-2\n",
        "# We changed n_estimator, maximum depth of the tree and max features which improved the accuracy of the model.\n",
        "# Increasing the number of trees or n_estimators allowed the model to capture more diverse patterns \n",
        "# and reduce the impact of individual noisy or biased trees. This helped to improve the overall \n",
        "# accuracy of the model.\n",
        "# The max-depth hyperparameter helped to balance the bias-variance trade-off in the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hX8oAEZ3QU3J",
        "outputId": "3162d4dc-e38a-45ed-b7eb-9e58411a96a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            "[[15  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  1  0  0\n",
            "   0  0]\n",
            " [ 0 18  0  0  0  1  0  2  1  0  0  0  0  0  0  0  0  0  2  0  0  0  0  0\n",
            "   0  0]\n",
            " [ 0  0 18  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0  0  1\n",
            "   0  0]\n",
            " [ 2  0  0 17  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  1  0  1\n",
            "   0  0]\n",
            " [ 0  1  0  0 19  1  0  0  1  0  0  0  0  0  0  0  0  0  1  0  0  0  1  0\n",
            "   0  0]\n",
            " [ 0  0  0  0  0 10  1  0  2  2  0  1  0  0  0  0  0  0  0  0  0  0  0  2\n",
            "   0  0]\n",
            " [ 0  0  0  0  1  0 19  0  1  0  0  0  1  0  0  0  0  0  0  1  0  0  0  1\n",
            "   0  0]\n",
            " [ 0  7  0  1  0  1  0 11  0  1  1  0  0  0  0  0  0  0  0  0  0  0  0  1\n",
            "   0  0]\n",
            " [ 0  0  0  0  0  0  0  0 18  2  0  0  0  0  0  0  0  0  1  0  0  0  0  0\n",
            "   0  0]\n",
            " [ 0  1  0  0  0  0  0  0  0 25  0  2  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0]\n",
            " [ 0  1  2  0  0  0  0  0  3  0 14  0  0  0  0  0  0  0  0  0  0  1  1  0\n",
            "   0  0]\n",
            " [ 0  0  1  0  0  0  0  0  1  1  0 19  1  0  0  0  0  0  0  0  0  0  0  1\n",
            "   0  0]\n",
            " [ 0  0  0  0  0  1  0  0  1  0  0  0 18  0  0  0  0  0  0  0  0  0  6  1\n",
            "   0  0]\n",
            " [ 0  0  0  0  2  0  0  0  0  0  0  1  1 16  2  0  1  0  0  0  1  0  0  0\n",
            "   0  0]\n",
            " [ 0  0  1  0  0  0  0  0  0  1  0  0  0  3 18  0  0  0  0  0  2  0  0  0\n",
            "   0  0]\n",
            " [ 0  0  0  0  0  0  0  0  1  0  0  1  0  0  0 22  0  0  0  0  0  0  0  0\n",
            "   0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1 13  0  0  0  0  0  0  0\n",
            "   0  0]\n",
            " [ 0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0 16  0  0  0  0  0  0\n",
            "   0  0]\n",
            " [ 0  0  0  0  0  1  0  0  2  1  0  0  1  0  0  0  0  0 16  0  0  0  0  0\n",
            "   0  0]\n",
            " [ 0  0  1  0  0  7  0  0  1  1  0  2  0  0  0  0  0  0  0  9  0  0  0  0\n",
            "   0  0]\n",
            " [ 0  2  1  1  0  0  0  0  0  0  0  0  0  2  0  0  0  0  0  0 13  0  0  0\n",
            "   0  0]\n",
            " [ 0  0  0  0  1  1  0  0  0  0  0  0  0  0  0  1  0  0  1  0  0 12  0  1\n",
            "   2  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  1  1  0  1  0  0  0  0  0  0  0  0  1 18  1\n",
            "   0  0]\n",
            " [ 0  0  0  1  0  3  0  0  1  2  0  2  0  0  0  0  0  0  0  0  0  0  1 18\n",
            "   0  0]\n",
            " [ 0  0  0  0  0  2  0  0  0  2  0  0  0  0  0  0  0  0  0  0  0  2  0  0\n",
            "  19  0]\n",
            " [ 0  0  0  0  0  0  1  0  0  1  0  4  0  0  0  0  0  0  0  0  0  0  0  1\n",
            "   0 22]]\n",
            "[('t', 0.42857142857142855), ('h', 0.4782608695652174), ('f', 0.5555555555555556), ('v', 0.631578947368421), ('k', 0.6363636363636364), ('x', 0.6428571428571429), ('m', 0.6666666666666666), ('n', 0.6666666666666666), ('u', 0.6842105263157895), ('o', 0.72), ('b', 0.75), ('z', 0.7586206896551724), ('y', 0.76), ('s', 0.7619047619047619), ('d', 0.7727272727272727), ('w', 0.782608695652174), ('e', 0.7916666666666666), ('g', 0.7916666666666666), ('l', 0.7916666666666666), ('c', 0.8571428571428571), ('i', 0.8571428571428571), ('a', 0.8823529411764706), ('j', 0.8928571428571429), ('p', 0.9166666666666666), ('q', 0.9285714285714286), ('r', 0.9411764705882353)]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Assuming rf_predictions contains the predicted labels from the Random Forest classifier\n",
        "\n",
        "confusion_matrix = confusion_matrix(val_y, rf_predictions)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix)\n",
        "\n",
        "# characters_list contains the list of characters corresponding to the labels\n",
        "\n",
        "characters_accuracy = {}\n",
        "characters_list = []\n",
        "for i in range(ord('a'), ord('z')+1):\n",
        "    characters_list.append(chr(i))\n",
        "\n",
        "\n",
        "for i in range(len(characters_list)):\n",
        "    true_positives = confusion_matrix[i, i]\n",
        "    total_instances = np.sum(confusion_matrix[i, :])\n",
        "    accuracy = true_positives / total_instances\n",
        "    characters_accuracy[characters_list[i]] = accuracy\n",
        "\n",
        "# Sorting characters based on accuracy in ascending order\n",
        "characters_accuracy_sorted = sorted(characters_accuracy.items(), key=lambda x: x[1])\n",
        "print(characters_accuracy_sorted)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "JCHv6WONQibT"
      },
      "outputs": [],
      "source": [
        "# ANSWER-4(A)\n",
        "# From the values observed, Random Forest Classifier has highest value of accuracy,\n",
        "# precision, recall, F1 score. So it is considered as Best Classifier.\n",
        "# Its confusion matrix is given as above.\n",
        "# ANSWER-4(B)\n",
        "# From the values obserevd, the characters 'h','u','f' have the least accuracy.\n",
        "# Note - the characters may vary for model to model as the accuracy value for lower \n",
        "# accuracy characters have minute difference in values.\n",
        "# ANSWER-4(C)\n",
        "# The reasons for this low accuracy could be-\n",
        "# Unbalanced Classes - If the dataset has imbalanced class distribution, where some\n",
        "#characters have significantly fewer instances compared to others, the classifier may\n",
        "# struggle to learn and accurately predict the minority class. The model may be\n",
        "# biased towards the majority class, resulting in lower accuracy for the minority\n",
        "# characters.\n",
        "# Limited Training Data - If there is an insufficient amount of training data available for\n",
        "# certain characters, it can be challenging for the classifier to learn their distinguishing\n",
        "# features accurately.\n",
        "# Underfitting or Overfitting - The model may suffer from overfitting, where it\n",
        "# memorizes the training data instead of learning general patterns, or underfitting,\n",
        "# where the model is too simple to capture the complexities of the data. Both\n",
        "# scenarios can result in reduced accuracy for certain characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "oPNJszTZY38N"
      },
      "outputs": [],
      "source": [
        "# ANSWER - 5\n",
        "# CNNs are composed of multiple layers that progressively learn and extract low-level\n",
        "# features. CNNs are also capable of learning transition-invariant features. They can \n",
        "# identify the character even if they appear in different positions and orientations. \n",
        "# Due to these factors we think building a convolutional neural network model for \n",
        "# character recognition can give higher accuracy then classifiers.\n",
        "# Below is the model we developed using basic nueral network algorithms that gave us a \n",
        "# little more accurate values than our best classifier. We can improve the basic \n",
        "# cnn model to get more higher accuracy values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5jWSKFMJjqq",
        "outputId": "0a2a62ee-ac97-4861-f615-1fcf9ce9e0db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "73/73 [==============================] - 7s 91ms/step - loss: 2.7504 - accuracy: 0.2247 - val_loss: 1.8336 - val_accuracy: 0.4949\n",
            "Epoch 2/10\n",
            "73/73 [==============================] - 7s 94ms/step - loss: 1.2776 - accuracy: 0.6344 - val_loss: 1.0510 - val_accuracy: 0.6969\n",
            "Epoch 3/10\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.7560 - accuracy: 0.7830 - val_loss: 0.9944 - val_accuracy: 0.6849\n",
            "Epoch 4/10\n",
            "73/73 [==============================] - 8s 108ms/step - loss: 0.4965 - accuracy: 0.8527 - val_loss: 0.8260 - val_accuracy: 0.7551\n",
            "Epoch 5/10\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.3220 - accuracy: 0.9148 - val_loss: 0.8894 - val_accuracy: 0.7603\n",
            "Epoch 6/10\n",
            "73/73 [==============================] - 7s 101ms/step - loss: 0.1908 - accuracy: 0.9478 - val_loss: 0.9058 - val_accuracy: 0.7654\n",
            "Epoch 7/10\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.0942 - accuracy: 0.9807 - val_loss: 1.0543 - val_accuracy: 0.7620\n",
            "Epoch 8/10\n",
            "73/73 [==============================] - 8s 106ms/step - loss: 0.0589 - accuracy: 0.9880 - val_loss: 1.1724 - val_accuracy: 0.7551\n",
            "Epoch 9/10\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.0997 - accuracy: 0.9803 - val_loss: 1.1023 - val_accuracy: 0.7637\n",
            "Epoch 10/10\n",
            "73/73 [==============================] - 7s 98ms/step - loss: 0.0366 - accuracy: 0.9949 - val_loss: 1.0799 - val_accuracy: 0.7808\n",
            "19/19 [==============================] - 0s 19ms/step - loss: 1.0799 - accuracy: 0.7808\n",
            "Validation Accuracy: 0.7808219194412231\n"
          ]
        }
      ],
      "source": [
        "# ANSWER - 5\n",
        "# Tried building a neural network  to enhance the accuracy of character recognition model.\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Step 1: Initialize and fit the LabelEncoder for training labels\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels_encoded = label_encoder.fit_transform(train_y)\n",
        "\n",
        "# Step 2: Initialize and fit the LabelEncoder for validation labels.\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "val_labels_encoded = label_encoder.fit_transform(val_y)\n",
        "\n",
        "# Step 3: Convert character labels to numerical values\n",
        "# The train_labels_encoded and val_labels_encoded obtained from the LabelEncoder are \n",
        "# then converted into one-hot encoded vectors using the to_categorical function from Keras.\n",
        "# This is necessary to represent the categorical labels in a format suitable for the model.\n",
        "train_labels_onehot = to_categorical(train_labels_encoded)\n",
        "val_labels_onehot = to_categorical(val_labels_encoded)\n",
        "\n",
        "# Step 4: Pre processing the data and reshaping\n",
        "train_x= train_x.reshape(-1,100,20,1)\n",
        "val_x = val_x.reshape(-1,100,20,1)\n",
        "\n",
        "# Step 5: Define the CNN model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(100,20,1)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(26, activation='softmax'))\n",
        "\n",
        "# Step 6: Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Step 7: Train the model\n",
        "model.fit(train_x, train_labels_onehot, batch_size=32, epochs=10, validation_data=(val_x, val_labels_onehot))\n",
        "\n",
        "# Step 8: Evaluate the model on validation data\n",
        "val_loss, val_accuracy = model.evaluate(val_x, val_labels_onehot)\n",
        "print(\"Validation Accuracy:\", val_accuracy)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
